{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匯入函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1595644668953,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "Gsw2jGsMobCT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pandas as pd # 引用套件並縮寫為 pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 參數設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1595644668954,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "quV9Md5A4lks"
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    max_sequence_length = 500 # 最長序列長度為n個字\n",
    "    min_word_frequency = 3 # 出現頻率小於n的話 ; 就當成罕見字\n",
    "    \n",
    "    vocab_size = None\n",
    "    category_num = None\n",
    "    \n",
    "    choose_model = 'lstm' # 想要使用的模型 ex lstm; rnn; gru\n",
    "    embedding_dim_size =300 # 詞向量維度\n",
    "    num_layer = 1 # 層數\n",
    "    num_units = [128] # 神經元\n",
    "    learning_rate = 0.0001 # 學習率         \n",
    "    keep_prob = 0.8 \n",
    "    \n",
    "    batch_size = 64 # mini-batch\n",
    "    epoch_size = 30 # epoch\n",
    "    \n",
    "    save_path = 'best_validation' # 模型儲存檔名\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERsKSkm9obCY"
   },
   "source": [
    "# 資料載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1595644762761,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "yz4TxhifobCY"
   },
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "with open(r'stop.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if len(line)>0:\n",
    "            stopwords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6154,
     "status": "ok",
     "timestamp": 1595644769452,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "Zfv4RorfobCc",
    "outputId": "026982b0-a41b-4d8c-9ef8-6bd5ad254fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "14000\n",
      "6000\n",
      "6000\n",
      "280\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 開啟 CSV 檔案\n",
    "data_train_x1,data_train_y1,data_valid_x1,data_valid_y1,data_test_x1,data_test_y1=[],[],[],[],[],[]\n",
    "\n",
    "with open(r'train.csv', newline='',encoding='utf-8') as csvfile:\n",
    "    # 讀取 CSV 檔案內容\n",
    "    rows = csv.reader(csvfile)\n",
    "    # 以迴圈輸出每一列\n",
    "    for row in rows:\n",
    "        data_train_x1.append(row[3])\n",
    "        data_train_x= data_train_x1[1:]\n",
    "        data_train_y1.append(row[5])\n",
    "        data_train_y=data_train_y1[1:]\n",
    "with open(r'test.csv', newline='',encoding='utf-8') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "      data_valid_x1.append(row[3])\n",
    "      data_valid_x=data_valid_x1[1:]\n",
    "      data_valid_y1.append(row[5])\n",
    "      data_valid_y=data_valid_y1[1:]\n",
    "with open(r'valid.csv', newline='',encoding='utf-8') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for row in rows:\n",
    "      data_test_x1.append(row[1])\n",
    "      data_test_x=data_test_x1[1:]\n",
    "      data_test_y1.append(row[2])\n",
    "      data_test_y=data_test_y1[1:]\n",
    "print(len(data_train_x))\n",
    "print(len(data_train_y))\n",
    "print(len(data_valid_x))\n",
    "print(len(data_valid_y))\n",
    "print(len(data_test_x))\n",
    "print(len(data_test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5774,
     "status": "ok",
     "timestamp": 1595644769454,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "q-jig49XwtBe"
   },
   "source": [
    "# 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6409,
     "status": "ok",
     "timestamp": 1595644770367,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "Mf-U3uQxobCf",
    "outputId": "d58065fb-2872-4ada-ab92-832c1a007423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "14000\n",
      "6000\n",
      "6000\n",
      "280\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'[^\\u4e00-\\u9fa5]+', '', text_string)\n",
    "    return(text_string)\n",
    "# Clean texts\n",
    "data_train_x = [clean_text(x) for x in data_train_x]\n",
    "data_valid_x = [clean_text(x) for x in data_valid_x]\n",
    "data_test_x = [clean_text(x) for x in data_test_x]\n",
    "print(len(data_train_x))\n",
    "print(len(data_train_y))\n",
    "print(len(data_valid_x))\n",
    "print(len(data_valid_y))\n",
    "print(len(data_test_x))\n",
    "print(len(data_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1595644770368,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "DQimlbMzyHbq",
    "outputId": "3e412615-2d2e-4f65-ecb2-3d3e9b712bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗前trian:14000,清洗前trian_target:14000\n",
      "清洗後trian:13413,清洗後trian_target:13413\n",
      "清洗前test:6000,清洗前test:6000\n",
      "清洗後trian:5804,清洗後trian_target:5804\n",
      "清洗前valid:280,清洗前valid:280\n",
      "清洗後trian:270,清洗後trian_target:270\n"
     ]
    }
   ],
   "source": [
    "clean_train_x = []\n",
    "clean_train_y = []\n",
    "clean_vaild_x = []\n",
    "clean_valid_y = []\n",
    "clean_test_x = []\n",
    "clean_test_y = []\n",
    "print(f'清洗前trian:{len(data_train_x)},清洗前trian_target:{len(data_train_y)}')\n",
    "for i in range(len(data_train_x)):\n",
    "    if data_train_x[i]!='':\n",
    "        clean_train_x.append(data_train_x[i])\n",
    "        clean_train_y.append(data_train_y[i])\n",
    "print(f'清洗後trian:{len(clean_train_x)},清洗後trian_target:{len(clean_train_y)}')\n",
    "\n",
    "print(f'清洗前test:{len(data_valid_x)},清洗前test:{len(data_valid_y)}')\n",
    "for i in range(len(data_valid_x)):\n",
    "    if data_valid_x[i]!='':\n",
    "        clean_vaild_x.append(data_valid_x[i])\n",
    "        clean_valid_y.append(data_valid_y[i])\n",
    "print(f'清洗後trian:{len(clean_vaild_x)},清洗後trian_target:{len(clean_valid_y)}')\n",
    "\n",
    "print(f'清洗前valid:{len(data_test_x)},清洗前valid:{len(data_test_y)}')\n",
    "for i in range(len(data_test_x)):\n",
    "    if data_test_x[i]!='':\n",
    "        clean_test_x.append(data_test_x[i])\n",
    "        clean_test_y.append(data_test_y[i])\n",
    "print(f'清洗後trian:{len(clean_test_x)},清洗後trian_target:{len(clean_test_y)}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_px0Q2JobCv"
   },
   "source": [
    "## 去除停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 367705,
     "status": "ok",
     "timestamp": 1595645188171,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "Hr0pUskKobCw",
    "outputId": "b1fe24dc-a8bd-4abf-ff33-7d3ad478858c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test data file is not exist\n"
     ]
    }
   ],
   "source": [
    "if(not os.path.isfile(\"seg_train_x.npy\")):\n",
    "    print(\"Train/Val/Test data file is not exist\")   \n",
    "    seg_train_x = []\n",
    "    seg_valid_x = []\n",
    "    seg_test_x = []\n",
    "    for i in range(len(clean_train_x)):\n",
    "        seg_train_x.append(' '.join([j for j in jieba.cut_for_search(clean_train_x[i]) if j not in stopwords]))\n",
    "    for i in range(len(clean_vaild_x)):\n",
    "        seg_valid_x.append(' '.join([j for j in jieba.cut_for_search(clean_vaild_x[i]) if j not in stopwords]))\n",
    "    for i in range(len(clean_test_x)):\n",
    "        seg_test_x.append(' '.join([j for j in jieba.cut_for_search(clean_test_x[i]) if j not in stopwords]))\n",
    "    seg_train_y = clean_train_y\n",
    "    seg_valid_y = clean_valid_y\n",
    "    seg_test_y = clean_test_y\n",
    "    np.save(\"seg_train_y\", seg_train_y)\n",
    "    np.save(\"seg_valid_y\", seg_valid_y)\n",
    "    np.save(\"seg_test_y\", seg_test_y)\n",
    "\n",
    "    np.save(\"seg_train_x\", seg_train_x)\n",
    "    np.save(\"seg_valid_x\", seg_valid_x)\n",
    "    np.save(\"seg_test_x\", seg_test_x)\n",
    "else:\n",
    "    print(\"Train/Val/Test data file is exist\")   \n",
    "    seg_train_x, seg_train_y = np.load(\"seg_train_x.npy\"),  np.load(\"seg_train_y.npy\")\n",
    "    seg_valid_x, seg_valid_y = np.load(\"seg_valid_x.npy\"),  np.load(\"seg_valid_y.npy\")\n",
    "    seg_test_x, seg_test_y = np.load(\"seg_test_x.npy\"),  np.load(\"seg_test_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16137,
     "status": "ok",
     "timestamp": 1595645225441,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "UyyMkpwrobCz",
    "outputId": "7475555b-7e02-4058-c4a3-75b12a718c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test data file is exist\n",
      ">> Train Data Shape : (13413, 500) ; Train Label Shape : (13413, 2)\n",
      ">> Val Data Shape : (5804, 500) ; Val Label Shape : (5804, 2)\n",
      ">> Test Data Shape : (270, 500) ; Test Label Shape : (270, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if(not os.path.isfile(\"train_x.npy\")):\n",
    "    print(\"Train/Val/Test data file is not exist\")   \n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(config.max_sequence_length, min_frequency=config.min_word_frequency)\n",
    "    train_x = np.array(list(vocab_processor.fit_transform(seg_train_x)))\n",
    "    train_y = tf.keras.utils.to_categorical(seg_train_y)\n",
    "    valid_x = np.array(list(vocab_processor.fit_transform(seg_valid_x)))\n",
    "    valid_y = tf.keras.utils.to_categorical(seg_valid_y)\n",
    "    test_x = np.array(list(vocab_processor.fit_transform(seg_test_x)))\n",
    "    test_y = tf.keras.utils.to_categorical(seg_test_y)\n",
    "    config.vocab_size = len(vocab_processor.vocabulary_)\n",
    "    \n",
    "    with open('vocab.txt', 'wt',encoding=\"utf-8\") as w_file:\n",
    "        for vocab in vocab_processor.vocabulary_._reverse_mapping:\n",
    "            w_file.write(vocab + \"\\n\")      \n",
    "    print(\"Total vocab size: {0}\".format(config.vocab_size))\n",
    "\n",
    "    np.save(\"train_x\", train_x); np.save(\"train_y\", train_y)\n",
    "    np.save(\"val_x\", valid_x) ; np.save(\"val_y\", valid_y)\n",
    "    np.save(\"test_x\", test_x) ; np.save(\"test_y\", test_y)\n",
    "else:\n",
    "    print(\"Train/Val/Test data file is exist\")   \n",
    "    train_x, train_y = np.load(\"train_x.npy\"),  np.load(\"train_y.npy\")\n",
    "    valid_x, valid_y = np.load(\"val_x.npy\"),  np.load(\"val_y.npy\")\n",
    "    test_x, test_y = np.load(\"test_x.npy\"),  np.load(\"test_y.npy\")\n",
    "    \n",
    "    config.vocab_size = sum(1 for line in open(\"vocab.txt\",encoding='utf-8'))\n",
    "\n",
    "config.category_num = train_y.shape[1]\n",
    "print('>> Train Data Shape : {0} ; Train Label Shape : {1}'.format(train_x.shape, train_y.shape))\n",
    "print('>> Val Data Shape : {0} ; Val Label Shape : {1}'.format(valid_x.shape, valid_y.shape))\n",
    "print('>> Test Data Shape : {0} ; Test Label Shape : {1}'.format(test_x.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIOdQtgoobC1"
   },
   "source": [
    "# 模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1595645300410,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "qwIZE7HEobC5"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class TextRNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # 四個等待輸入的data\n",
    "        self.batch_size = tf.placeholder(tf.int32, [] , name = 'batch_size')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name = 'keep_prob')\n",
    "        \n",
    "        # Initial\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.config.max_sequence_length] , name = 'x')\n",
    "        self.y_label = tf.placeholder(tf.float32, [None, self.config.category_num], name = 'y_label')\n",
    "        self.choose_model = config.choose_model\n",
    "        self.rnn()\n",
    "    # Get LSTM Cell\n",
    "    def cell(self, num_units):\n",
    "        #BasicLSTMCell activity => default tanh\n",
    "        if self.choose_model == \"lstm\":\n",
    "            #可以設定peephole等屬性\n",
    "            LSTM_cell = rnn.LSTMCell( initializer = tf.random_uniform_initializer(-0.1, 0.1,seed=2 )) \n",
    "        elif self.choose_model == \"basic\":\n",
    "            #最基礎的，沒有peephole\n",
    "            LSTM_cell = rnn.BasicLSTMCell(num_units = num_units, forget_bias = 1.0, state_is_tuple = True) \n",
    "        else:\n",
    "            LSTM_cell = rnn.GRUCell(num_units)\n",
    "\n",
    "        return rnn.DropoutWrapper(LSTM_cell, output_keep_prob = self.keep_prob)\n",
    "    \n",
    "    def rnn(self):\n",
    "        \"\"\"RNN模型\"\"\"\n",
    "        # 詞向量映射\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim_size])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.x)\n",
    "            \n",
    "        # RNN Layers\n",
    "        with tf.name_scope('layers'):\n",
    "            with tf.name_scope('RNN'):\n",
    "                LSTM_cells = rnn.MultiRNNCell([self.cell(int(self.config.num_units[_])) for _ in range(self.config.num_layer)])\n",
    "                # x_shape = tf.reshape(self.x, [-1, self.config.truncate, self.config.vectorSize])\n",
    "                \n",
    "            with tf.name_scope('output'):\n",
    "                init_state = LSTM_cells.zero_state(self.batch_size, dtype = tf.float32)\n",
    "                outputs, final_state = tf.nn.dynamic_rnn(LSTM_cells, inputs = embedding_inputs, \n",
    "                                                        initial_state = init_state, time_major = False, dtype = tf.float32)\n",
    "                \n",
    "        # Output Layer\n",
    "        with tf.name_scope('output_layer'):\n",
    "            # 全連接層，後面接dropout以及relu激活\n",
    "            fc1 = tf.layers.dense(outputs[:, -1, :], int(self.config.num_units[len(self.config.num_units)-1]))\n",
    "            fc1 = tf.contrib.layers.dropout(fc1, self.keep_prob)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "                \n",
    "            # 分類器\n",
    "            y = tf.layers.dense(fc1, self.config.category_num, name = 'y')\n",
    "        \n",
    "        self.y_pred_cls = tf.argmax(y, axis = 1) #預測類別\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            with tf.name_scope('total'):\n",
    "                self.y=tf.nn.softmax(y)   \n",
    "                self.softmax = tf.nn.softmax_cross_entropy_with_logits(labels = self.y_label, logits = y)\n",
    "                self.cross_entropy = tf.reduce_mean(self.softmax)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.config.learning_rate).minimize(self.cross_entropy)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                self.correction_prediction = tf.equal(self.y_pred_cls, tf.argmax(self.y_label, axis = 1))\n",
    "            with tf.name_scope('accuracy'):\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(self.correction_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmHw-Sn2obC7"
   },
   "source": [
    "# 建立LSTM網路訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2013613,
     "status": "ok",
     "timestamp": 1595647314018,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "a9-mc2RqobC-",
    "outputId": "8dc80130-3355-4315-f9c2-9ee31fd95df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of best_validation doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, dense/bias, dense/bias/Adam, dense/bias/Adam_1, dense/kernel, dense/kernel/Adam, dense/kernel/Adam_1, embedding, embedding/Adam, embedding/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/bias, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, train/beta1_power, train/beta2_power, y/bias, y/bias/Adam, y/bias/Adam_1, y/kernel, y/kernel/Adam, y/kernel/Adam_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1592\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1594\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, dense/bias, dense/bias/Adam, dense/bias/Adam_1, dense/kernel, dense/kernel/Adam, dense/kernel/Adam_1, embedding, embedding/Adam, embedding/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/bias, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, train/beta1_power, train/beta2_power, y/bias, y/bias/Adam, y/bias/Adam_1, y/kernel, y/kernel/Adam, y/kernel/Adam_1)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 597, in start\n    self.io_loop.start()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-8bf477584bc1>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1239, in __init__\n    self.build()\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1248, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1284, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 762, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 297, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 240, in save_op\n    tensors)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1286, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\629\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Failed to create a directory: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, dense/bias, dense/bias/Adam, dense/bias/Adam_1, dense/kernel, dense/kernel/Adam, dense/kernel/Adam_1, embedding, embedding/Adam, embedding/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/bias, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, train/beta1_power, train/beta2_power, y/bias, y/bias/Adam, y/bias/Adam_1, y/kernel, y/kernel/Adam, y/kernel/Adam_1)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8bf477584bc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0mbest_val_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[0mlast_improved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                     \u001b[0mimproved_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\chris_deeplearn\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1612\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1613\u001b[0m                   save_path))\n\u001b[1;32m-> 1614\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of best_validation doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"得到已使用時間\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    \n",
    "    return timedelta(seconds = int(round(time_dif)))\n",
    "\n",
    "def feedData(x_batch, y_batch, keep_prob, batch_size, model):\n",
    "    feed_dict = {\n",
    "        model.x: x_batch,\n",
    "        model.y_label: y_batch,\n",
    "        model.keep_prob: keep_prob,\n",
    "        model.batch_size: batch_size\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "best_val_acc = -1.0 # 最佳驗證集準確度\n",
    "last_improved = 0 # 紀錄上一次提升batch \n",
    "require_improvement = 300  # 如果超过n輪未提升，提前结束訓練\n",
    "total_batch = 0  # 總批次\n",
    "print_per_batch = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = TextRNN(config)\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    flag = False\n",
    "    for epoch in range(config.epoch_size):\n",
    "        print('Epoch: {0}'.format(epoch + 1))\n",
    "        shuffled_ix = np.random.permutation(np.arange(len(train_x)))\n",
    "        train_x = train_x[shuffled_ix]\n",
    "        train_y = train_y[shuffled_ix]\n",
    "        for step in range(0, train_x.shape[0], config.batch_size):\n",
    "            batch_x, batch_y = train_x[step:step + config.batch_size], train_y[step:step + config.batch_size]\n",
    "            \n",
    "            if total_batch % print_per_batch == 0:  \n",
    "                train_loss, train_acc = sess.run([model.cross_entropy, model.accuracy], feed_dict = feedData(batch_x, batch_y, 1.0, batch_x.shape[0], model))\n",
    "                val_loss, val_acc = sess.run([model.cross_entropy, model.accuracy], feed_dict = feedData(valid_x, valid_y, 1.0, valid_x.shape[0], model))\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess = sess, save_path = config.save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "                    \n",
    "                time_dif = get_time_dif(start_time)                               \n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.3}, Train Acc: {2:>7.2%}, Val Loss: {3:>6.3}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, train_loss, train_acc, val_loss, val_acc, time_dif, improved_str))\n",
    "            \n",
    "            # train\n",
    "            sess.run(model.train_step, feed_dict = feedData(batch_x, batch_y, 1.0, batch_x.shape[0], model))\n",
    "            total_batch += 1\n",
    "            \n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 驗證集準確度長期不提升，提前结束訓練\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "    print(\"訓練完成...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5768,
     "status": "ok",
     "timestamp": 1595647346846,
     "user": {
      "displayName": "蔡孟廷",
      "photoUrl": "",
      "userId": "16683234324658169721"
     },
     "user_tz": -480
    },
    "id": "hOKaXmp-obDB",
    "outputId": "ba626090-3244-4782-9e89-3512ecc5104f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-3f9fdca72f28>:64: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from best_validation\n",
      "Test Loss:   0.37, Test Acc:  88.89%, Time: 0:00:05\n",
      "測試完成...\n",
      ">> Confusion Matrix...\n",
      "[[123  17]\n",
      " [ 13 117]]\n",
      "[[0.85457957 0.14542039]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457945 0.14542061]\n",
      " [0.17786965 0.8221304 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542034]\n",
      " [0.20114186 0.79885817]\n",
      " [0.85457957 0.14542034]\n",
      " [0.15100738 0.8489926 ]\n",
      " [0.18927203 0.81072795]\n",
      " [0.19445007 0.8055499 ]\n",
      " [0.8545493  0.14545067]\n",
      " [0.18955106 0.8104489 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.18276401 0.817236  ]\n",
      " [0.85457957 0.14542037]\n",
      " [0.18314867 0.8168513 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.18995462 0.81004536]\n",
      " [0.85457957 0.14542039]\n",
      " [0.19845487 0.8015452 ]\n",
      " [0.85442257 0.1455774 ]\n",
      " [0.8545767  0.14542328]\n",
      " [0.19919348 0.80080646]\n",
      " [0.1974991  0.8025009 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.1919054  0.8080946 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19061029 0.8093897 ]\n",
      " [0.13289246 0.8671075 ]\n",
      " [0.8545795  0.14542048]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19982645 0.8001735 ]\n",
      " [0.36553133 0.6344686 ]\n",
      " [0.19393902 0.80606097]\n",
      " [0.18761109 0.81238896]\n",
      " [0.1866826  0.8133174 ]\n",
      " [0.19720466 0.80279535]\n",
      " [0.18500975 0.8149902 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542034]\n",
      " [0.18950103 0.810499  ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542039]\n",
      " [0.8545547  0.14544527]\n",
      " [0.18477829 0.8152217 ]\n",
      " [0.18495019 0.81504977]\n",
      " [0.85457957 0.14542034]\n",
      " [0.1834289  0.8165711 ]\n",
      " [0.26659846 0.73340154]\n",
      " [0.85457957 0.14542039]\n",
      " [0.18769078 0.81230927]\n",
      " [0.85457957 0.14542037]\n",
      " [0.8545788  0.14542116]\n",
      " [0.85457927 0.1454208 ]\n",
      " [0.20464562 0.79535437]\n",
      " [0.85457957 0.14542034]\n",
      " [0.2830286  0.71697134]\n",
      " [0.19232245 0.8076775 ]\n",
      " [0.85457957 0.1454204 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542043]\n",
      " [0.85457957 0.14542039]\n",
      " [0.23802087 0.7619791 ]\n",
      " [0.85457957 0.14542037]\n",
      " [0.2415842  0.7584158 ]\n",
      " [0.18339448 0.8166055 ]\n",
      " [0.75049704 0.24950294]\n",
      " [0.28409797 0.71590203]\n",
      " [0.18578632 0.81421375]\n",
      " [0.35562894 0.64437103]\n",
      " [0.85457945 0.14542055]\n",
      " [0.8545797  0.14542036]\n",
      " [0.85457957 0.14542034]\n",
      " [0.18957032 0.8104297 ]\n",
      " [0.854579   0.14542104]\n",
      " [0.19961396 0.800386  ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.8521225  0.14787751]\n",
      " [0.85457957 0.14542039]\n",
      " [0.2709494  0.72905064]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457927 0.1454208 ]\n",
      " [0.08793443 0.91206557]\n",
      " [0.19184868 0.8081513 ]\n",
      " [0.20740053 0.79259944]\n",
      " [0.8416471  0.15835296]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.1454204 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.1848746  0.8151254 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.56025577 0.43974414]\n",
      " [0.22964057 0.77035946]\n",
      " [0.1944659  0.80553406]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19694301 0.803057  ]\n",
      " [0.2403082  0.7596918 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19384344 0.8061566 ]\n",
      " [0.85457957 0.14542043]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542039]\n",
      " [0.25310642 0.74689364]\n",
      " [0.85457945 0.14542055]\n",
      " [0.85457957 0.14542034]\n",
      " [0.18828312 0.8117169 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.21343888 0.7865611 ]\n",
      " [0.19265437 0.8073457 ]\n",
      " [0.85457957 0.1454204 ]\n",
      " [0.18744041 0.81255955]\n",
      " [0.8493357  0.15066431]\n",
      " [0.20740189 0.7925981 ]\n",
      " [0.1111857  0.8888143 ]\n",
      " [0.1851606  0.8148394 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.20193005 0.79807   ]\n",
      " [0.19104993 0.80895   ]\n",
      " [0.25785032 0.74214965]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542037]\n",
      " [0.8545795  0.1454205 ]\n",
      " [0.19715264 0.8028473 ]\n",
      " [0.19493622 0.8050637 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.1454204 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19415891 0.80584115]\n",
      " [0.85457957 0.14542037]\n",
      " [0.18971181 0.8102882 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.20062417 0.7993759 ]\n",
      " [0.19026244 0.80973756]\n",
      " [0.2154985  0.7845015 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542037]\n",
      " [0.85457957 0.14542034]\n",
      " [0.18661952 0.81338054]\n",
      " [0.20224245 0.79775757]\n",
      " [0.1860184  0.8139816 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.8545795  0.14542048]\n",
      " [0.20197923 0.7980208 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.21255314 0.7874468 ]\n",
      " [0.19679798 0.80320203]\n",
      " [0.1954946  0.80450547]\n",
      " [0.2107335  0.78926647]\n",
      " [0.18335582 0.81664413]\n",
      " [0.18533035 0.81466967]\n",
      " [0.20713635 0.7928636 ]\n",
      " [0.8514099  0.14859015]\n",
      " [0.8545562  0.14544384]\n",
      " [0.85457957 0.14542039]\n",
      " [0.1921038  0.80789614]\n",
      " [0.85457957 0.14542037]\n",
      " [0.21365158 0.78634846]\n",
      " [0.20681357 0.7931864 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19850652 0.80149347]\n",
      " [0.20164628 0.7983537 ]\n",
      " [0.20036376 0.79963624]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.19049644 0.8095036 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.8545762  0.14542383]\n",
      " [0.18979199 0.810208  ]\n",
      " [0.21273044 0.7872696 ]\n",
      " [0.85401565 0.1459844 ]\n",
      " [0.32359862 0.6764014 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.8545795  0.1454205 ]\n",
      " [0.1985681  0.80143183]\n",
      " [0.19132    0.80868   ]\n",
      " [0.8179321  0.18206787]\n",
      " [0.19225463 0.8077454 ]\n",
      " [0.24045727 0.75954276]\n",
      " [0.85457957 0.14542034]\n",
      " [0.8545775  0.14542249]\n",
      " [0.19494644 0.80505353]\n",
      " [0.19045982 0.8095402 ]\n",
      " [0.16626146 0.83373857]\n",
      " [0.17734109 0.82265896]\n",
      " [0.1957564  0.80424356]\n",
      " [0.85457957 0.14542034]\n",
      " [0.07914881 0.9208512 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.8518626  0.14813744]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542034]\n",
      " [0.8545795  0.14542048]\n",
      " [0.19527158 0.8047284 ]\n",
      " [0.8545678  0.14543214]\n",
      " [0.85457957 0.14542037]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19858615 0.80141383]\n",
      " [0.20097092 0.79902905]\n",
      " [0.49527374 0.50472635]\n",
      " [0.28494015 0.7150598 ]\n",
      " [0.17950012 0.8204999 ]\n",
      " [0.2365469  0.76345307]\n",
      " [0.85457957 0.14542039]\n",
      " [0.18978663 0.8102133 ]\n",
      " [0.1928861  0.8071139 ]\n",
      " [0.19747922 0.8025208 ]\n",
      " [0.20866078 0.7913392 ]\n",
      " [0.18010475 0.8198952 ]\n",
      " [0.8545793  0.14542067]\n",
      " [0.18633872 0.8136613 ]\n",
      " [0.8545788  0.14542119]\n",
      " [0.21058117 0.7894189 ]\n",
      " [0.85217786 0.14782213]\n",
      " [0.22928485 0.7707152 ]\n",
      " [0.19449206 0.80550796]\n",
      " [0.85457957 0.14542039]\n",
      " [0.17611791 0.82388204]\n",
      " [0.26074794 0.73925203]\n",
      " [0.18535529 0.8146447 ]\n",
      " [0.19612463 0.8038753 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.20364629 0.79635376]\n",
      " [0.206575   0.793425  ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.19615088 0.80384916]\n",
      " [0.19942379 0.8005762 ]\n",
      " [0.26747534 0.73252463]\n",
      " [0.20472805 0.79527193]\n",
      " [0.85457945 0.14542061]\n",
      " [0.85457927 0.14542072]\n",
      " [0.17005023 0.8299498 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.27106744 0.7289325 ]\n",
      " [0.85457957 0.14542039]\n",
      " [0.20367788 0.79632217]\n",
      " [0.85457957 0.14542034]\n",
      " [0.85457957 0.14542039]\n",
      " [0.854579   0.14542104]\n",
      " [0.22024637 0.7797536 ]\n",
      " [0.85457957 0.14542034]\n",
      " [0.8545789  0.1454211 ]\n",
      " [0.71734744 0.28265256]\n",
      " [0.19383101 0.806169  ]\n",
      " [0.8545784  0.14542161]\n",
      " [0.15985548 0.8401445 ]\n",
      " [0.19372846 0.80627155]\n",
      " [0.85457957 0.14542039]\n",
      " [0.85457957 0.14542039]\n",
      " [0.23942398 0.76057607]\n",
      " [0.1887295  0.8112705 ]\n",
      " [0.85457957 0.14542037]\n",
      " [0.27726215 0.72273785]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"得到已使用時間\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    \n",
    "    return timedelta(seconds = int(round(time_dif)))\n",
    "def feedData(x_batch, y_batch, keep_prob, batch_size, model):\n",
    "    feed_dict = {\n",
    "        model.x: x_batch,\n",
    "        model.y_label: y_batch,\n",
    "        model.keep_prob: keep_prob,\n",
    "        model.batch_size: batch_size\n",
    "    }\n",
    "    return feed_dict\n",
    "tf.reset_default_graph()\n",
    "model = TextRNN(config)\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess = sess, save_path = config.save_path)  # 讀取保存的模型\n",
    "    shuffled_ix = np.random.permutation(np.arange(len(test_x)))\n",
    "    test_x = test_x[shuffled_ix]\n",
    "    test_y = test_y[shuffled_ix]\n",
    "    test_loss, test_acc, test_predict_label,y,y_label = sess.run([model.cross_entropy, model.accuracy, model.y_pred_cls,model.y,model.y_label], feed_dict = feedData(test_x, test_y, 1.0, test_x.shape[0], model))\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}, Time: {2}'\n",
    "    print(msg.format(test_loss, test_acc, time_dif))\n",
    "    print(\"測試完成...\") \n",
    "    test_label = np.argmax(test_y, 1)\n",
    "    # 混淆矩陣\n",
    "    print(\">> Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(test_label, test_predict_label)\n",
    "    print(cm)\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_test2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
